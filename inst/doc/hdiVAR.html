<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />

<meta name="viewport" content="width=device-width, initial-scale=1" />

<meta name="author" content="Xiang Lyu" />


<title>Vignette of R package hdiVAR</title>

<script>// Pandoc 2.9 adds attributes on both header and div. We remove the former (to
// be compatible with the behavior of Pandoc < 2.8).
document.addEventListener('DOMContentLoaded', function(e) {
  var hs = document.querySelectorAll("div.section[class*='level'] > :first-child");
  var i, h, a;
  for (i = 0; i < hs.length; i++) {
    h = hs[i];
    if (!/^h[1-6]$/i.test(h.tagName)) continue;  // it should be a header h1-h6
    a = h.attributes;
    while (a.length > 0) h.removeAttribute(a[0].name);
  }
});
</script>

<style type="text/css">
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
span.underline{text-decoration: underline;}
div.column{display: inline-block; vertical-align: top; width: 50%;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
</style>



<style type="text/css">
code {
white-space: pre;
}
.sourceCode {
overflow: visible;
}
</style>
<style type="text/css" data-origin="pandoc">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
{ counter-reset: source-line 0; }
pre.numberSource code > span
{ position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
{ content: counter(source-line);
position: relative; left: -1em; text-align: right; vertical-align: baseline;
border: none; display: inline-block;
-webkit-touch-callout: none; -webkit-user-select: none;
-khtml-user-select: none; -moz-user-select: none;
-ms-user-select: none; user-select: none;
padding: 0 4px; width: 4em;
color: #aaaaaa;
}
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa; padding-left: 4px; }
div.sourceCode
{ }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } 
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } 
code span.at { color: #7d9029; } 
code span.bn { color: #40a070; } 
code span.bu { color: #008000; } 
code span.cf { color: #007020; font-weight: bold; } 
code span.ch { color: #4070a0; } 
code span.cn { color: #880000; } 
code span.co { color: #60a0b0; font-style: italic; } 
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } 
code span.do { color: #ba2121; font-style: italic; } 
code span.dt { color: #902000; } 
code span.dv { color: #40a070; } 
code span.er { color: #ff0000; font-weight: bold; } 
code span.ex { } 
code span.fl { color: #40a070; } 
code span.fu { color: #06287e; } 
code span.im { color: #008000; font-weight: bold; } 
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } 
code span.kw { color: #007020; font-weight: bold; } 
code span.op { color: #666666; } 
code span.ot { color: #007020; } 
code span.pp { color: #bc7a00; } 
code span.sc { color: #4070a0; } 
code span.ss { color: #bb6688; } 
code span.st { color: #4070a0; } 
code span.va { color: #19177c; } 
code span.vs { color: #4070a0; } 
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } 
</style>
<script>
// apply pandoc div.sourceCode style to pre.sourceCode instead
(function() {
  var sheets = document.styleSheets;
  for (var i = 0; i < sheets.length; i++) {
    if (sheets[i].ownerNode.dataset["origin"] !== "pandoc") continue;
    try { var rules = sheets[i].cssRules; } catch (e) { continue; }
    var j = 0;
    while (j < rules.length) {
      var rule = rules[j];
      // check if there is a div.sourceCode rule
      if (rule.type !== rule.STYLE_RULE || rule.selectorText !== "div.sourceCode") {
        j++;
        continue;
      }
      var style = rule.style.cssText;
      // check if color or background-color is set
      if (rule.style.color === '' && rule.style.backgroundColor === '') {
        j++;
        continue;
      }
      // replace div.sourceCode by a pre.sourceCode rule
      sheets[i].deleteRule(j);
      sheets[i].insertRule('pre.sourceCode{' + style + '}', j);
    }
  }
})();
</script>




<style type="text/css">body {
background-color: #fff;
margin: 1em auto;
max-width: 700px;
overflow: visible;
padding-left: 2em;
padding-right: 2em;
font-family: "Open Sans", "Helvetica Neue", Helvetica, Arial, sans-serif;
font-size: 14px;
line-height: 1.35;
}
#TOC {
clear: both;
margin: 0 0 10px 10px;
padding: 4px;
width: 400px;
border: 1px solid #CCCCCC;
border-radius: 5px;
background-color: #f6f6f6;
font-size: 13px;
line-height: 1.3;
}
#TOC .toctitle {
font-weight: bold;
font-size: 15px;
margin-left: 5px;
}
#TOC ul {
padding-left: 40px;
margin-left: -1.5em;
margin-top: 5px;
margin-bottom: 5px;
}
#TOC ul ul {
margin-left: -2em;
}
#TOC li {
line-height: 16px;
}
table {
margin: 1em auto;
border-width: 1px;
border-color: #DDDDDD;
border-style: outset;
border-collapse: collapse;
}
table th {
border-width: 2px;
padding: 5px;
border-style: inset;
}
table td {
border-width: 1px;
border-style: inset;
line-height: 18px;
padding: 5px 5px;
}
table, table th, table td {
border-left-style: none;
border-right-style: none;
}
table thead, table tr.even {
background-color: #f7f7f7;
}
p {
margin: 0.5em 0;
}
blockquote {
background-color: #f6f6f6;
padding: 0.25em 0.75em;
}
hr {
border-style: solid;
border: none;
border-top: 1px solid #777;
margin: 28px 0;
}
dl {
margin-left: 0;
}
dl dd {
margin-bottom: 13px;
margin-left: 13px;
}
dl dt {
font-weight: bold;
}
ul {
margin-top: 0;
}
ul li {
list-style: circle outside;
}
ul ul {
margin-bottom: 0;
}
pre, code {
background-color: #f7f7f7;
border-radius: 3px;
color: #333;
white-space: pre-wrap; 
}
pre {
border-radius: 3px;
margin: 5px 0px 10px 0px;
padding: 10px;
}
pre:not([class]) {
background-color: #f7f7f7;
}
code {
font-family: Consolas, Monaco, 'Courier New', monospace;
font-size: 85%;
}
p > code, li > code {
padding: 2px 0px;
}
div.figure {
text-align: center;
}
img {
background-color: #FFFFFF;
padding: 2px;
border: 1px solid #DDDDDD;
border-radius: 3px;
border: 1px solid #CCCCCC;
margin: 0 5px;
}
h1 {
margin-top: 0;
font-size: 35px;
line-height: 40px;
}
h2 {
border-bottom: 4px solid #f7f7f7;
padding-top: 10px;
padding-bottom: 2px;
font-size: 145%;
}
h3 {
border-bottom: 2px solid #f7f7f7;
padding-top: 10px;
font-size: 120%;
}
h4 {
border-bottom: 1px solid #f7f7f7;
margin-left: 8px;
font-size: 105%;
}
h5, h6 {
border-bottom: 1px solid #ccc;
font-size: 105%;
}
a {
color: #0033dd;
text-decoration: none;
}
a:hover {
color: #6666ff; }
a:visited {
color: #800080; }
a:visited:hover {
color: #BB00BB; }
a[href^="http:"] {
text-decoration: underline; }
a[href^="https:"] {
text-decoration: underline; }

code > span.kw { color: #555; font-weight: bold; } 
code > span.dt { color: #902000; } 
code > span.dv { color: #40a070; } 
code > span.bn { color: #d14; } 
code > span.fl { color: #d14; } 
code > span.ch { color: #d14; } 
code > span.st { color: #d14; } 
code > span.co { color: #888888; font-style: italic; } 
code > span.ot { color: #007020; } 
code > span.al { color: #ff0000; font-weight: bold; } 
code > span.fu { color: #900; font-weight: bold; } 
code > span.er { color: #a61717; background-color: #e3d2d2; } 
</style>




</head>

<body>




<h1 class="title toc-ignore">Vignette of R package hdiVAR</h1>
<h4 class="author">Xiang Lyu</h4>
<h4 class="date">Sep 25, 2020</h4>



<p><a id="top"></a></p>
<blockquote>
<p><a href="#basic">Basic info</a></p>
</blockquote>
<blockquote>
<p><a href="#setup">Problem setup</a></p>
</blockquote>
<blockquote>
<p><a href="#method">Methodology</a></p>
</blockquote>
<blockquote>
<p><a href="#example">Quick Start</a></p>
</blockquote>
<blockquote>
<p><a href="#ref">Reference</a></p>
</blockquote>
<p><a id="basic"></a></p>
<div id="basic-info" class="section level1">
<h1>Basic Info</h1>
<p>This package considers the estimation and statistical inference of
high-dimensional vector autoregression with measurement error, also
known as linear gaussian state-space model. A sparse
expectation-maximization (EM) algorithm is provided for parameter
estimation. For transition matrix inference, both global testing and
simultaneous testing are implemented, with consistent size and false
discovery rate (FDR) control. The methods are proposed in <a href="https://arxiv.org/abs/2009.08011">Lyu et al.Â (2020)</a>.</p>
<p><a id="setup"></a></p>
</div>
<div id="problem-setup" class="section level1">
<h1>Problem setup</h1>
<p>The model of interest is high-dimensional vector autoregression (VAR)
with measurement error, <span class="math display">\[
\mathbf{y}_{t} =  \mathbf{x}_{t} + \mathbf{\epsilon}_{t}, \ \ \ \     
\mathbf{x}_{t+1}  =  \mathbf{A}_* \mathbf{x}_{t}  + \mathbf{\eta}_{t},
\]</span> where <span class="math inline">\(\mathbf{y}_{t} \in
\mathbb{R}^{p}\)</span> is the observed multivariate time series, <span class="math inline">\(\mathbf{x}_{t}\in \mathbb{R}^{p}\)</span> is the
multivariate latent signal that admits an autoregressive structure,
<span class="math inline">\(\mathbf{\epsilon}_{t}\in
\mathbb{R}^{p}\)</span> is the measurement error for the observed time
series, <span class="math inline">\(\mathbf{\eta}_{t} \in
\mathbb{R}^{p}\)</span> is the white noise of the latent signal, and
<span class="math inline">\(\mathbf{A}_*\in \mathbb{R}^{p\times
p}\)</span> is the sparse transition matrix that encodes the directional
relations among the latent signal variables of <span class="math inline">\(\mathbf{x}_{t}\)</span>. Furthermore, we focus on
the scenario <span class="math inline">\(\|\mathbf{A}_*\|_2
&lt;1\)</span> such that the VAR model of <span class="math inline">\(\mathbf{x}_{t}\)</span> is stationary. The error
terms <span class="math inline">\(\mathbf{\epsilon}_{t}\)</span> and
<span class="math inline">\(\mathbf{\eta}_{t}\)</span> are
i.i.d.Â multivariate normal with mean zero and covariance <span class="math inline">\(\sigma_{\epsilon,*}^2 \mathbf{I}_p\)</span> and
<span class="math inline">\(\sigma_{\eta,*}^2 \mathbf{I}_p\)</span>,
respectively, and are independent of <span class="math inline">\(\mathbf{x}_{t}\)</span>. This package can handle
high-dimensional setting where <span class="math inline">\(p^2\)</span>
exceeds the length of series <span class="math inline">\(T\)</span>.</p>
<p>Estimation aims to recover <span class="math inline">\(\{\mathbf{A}_*, \sigma_{\eta,*}^2,
\sigma_{\epsilon,*}^2\}\)</span> from observation <span class="math inline">\(\mathbf{y}_{t}\)</span>âs. The statistical
inference goal is the transition matrix <span class="math inline">\(\mathbf{A}_*\)</span>. The global hypotheses is
<span class="math display">\[
H_{0}: A_{*,ij} = A_{0,ij}, \  \textrm{ for all } (i,j) \in \mathcal{S}
\quad \textrm{versus} \quad  H_{1}: A_{*,ij} \neq A_{0,ij},  \ \textrm{
for some } (i,j) \in \mathcal{S},
\]</span> for a given <span class="math inline">\(\mathbf{A}_{0} =
(A_{0,ij}) \in \mathbb{R}^{p \times p}\)</span> and <span class="math inline">\(\mathcal{S} \subseteq [p] \times [p]\)</span>,
where <span class="math inline">\([p] = \{1, \ldots, p\}\)</span>. The
most common choice is <span class="math inline">\(\mathbf{A}_0=\mathbf{0}_{p\times p}\)</span> and
<span class="math inline">\(\mathcal{S} =[p] \times [p]\)</span>. The
simultaneous hypotheses are <span class="math display">\[
H_{0; ij}: A_{*,ij} = A_{0,ij},  \quad \textrm{versus} \quad  H_{1; ij}:
A_{*,ij} \ne A_{0,ij},  \ \textrm{ for all } (i, j) \in \mathcal{S}.
\]</span></p>
<p><a id="method"></a></p>
</div>
<div id="methodology" class="section level1">
<h1>Methodology</h1>
<div id="estimation-sparse-em-algorithm" class="section level2">
<h2>1. Estimation: sparse EM algorithm</h2>
<p>Let <span class="math inline">\(\{ \mathbf{y}_{t},\mathbf{x}_{t}
\}_{t=1}^{T}\)</span> denote the complete data, where <span class="math inline">\(T\)</span> is the total number of observations,
<span class="math inline">\(\mathbf{y}_{t}\)</span> is observed but
<span class="math inline">\(\mathbf{x}_{t}\)</span> is latent. Let <span class="math inline">\(\Theta = \left\{ \mathbf{A}, \sigma_{\eta}^2,
\sigma_{\epsilon}^2 \right\}\)</span> collect all the parameters of
interest in model <span class="math inline">\(\eqref{eq:
model_measure}\)</span>, and <span class="math inline">\(\Theta_* =
\left\{ \mathbf{A}_*, \sigma_{\eta,*}^2, \sigma_{\epsilon,*}^2
\right\}\)</span> denote the true parameters. The goal is to estimate
<span class="math inline">\(\Theta_*\)</span> by maximizing the
log-likelihood function of the observed data, <span class="math inline">\(\ell (\Theta |
\{\mathbf{y}_{t}\}_{t=1}^T)\)</span>, with respect to <span class="math inline">\(\Theta\)</span>. The computation of <span class="math inline">\(\ell (\Theta |
\{\mathbf{y}_{t}\}_{t=1}^T)\)</span>, however, is highly nontrivial.
Sparse EM algorithm then turns to an auxiliary function, named the
finite-sample <span class="math inline">\(Q\)</span>-function, <span class="math display">\[
Q_y (\Theta | \Theta&#39;) = \mathbb{E} \left[ \ell\left( \Theta | \{
\mathbf{y}_{t},\mathbf{x}_{t} \}_{t=1}^{T} \right) | \{
\mathbf{y}_{t}\}_{t=1}^T, \Theta&#39; \right],
\]</span> which is defined as the expectation of the log-likelihood
function for the complete data <span class="math inline">\(\ell(\Theta |
\{ \mathbf{y}_{t},\mathbf{x}_{t} \}_{t=1}^{T})\)</span>, conditioning on
a parameter set <span class="math inline">\(\Theta&#39;\)</span> and the
observed data <span class="math inline">\(\mathbf{y}_t\)</span>, and the
expectation is taken with respect to the latent data <span class="math inline">\(\mathbf{x}_t\)</span>. The <span class="math inline">\(Q\)</span>-function can be computed efficiently by
Kalman filter and smoothing, and provides a lower bound of the target
log-likelihood function <span class="math inline">\(\ell
(\Theta|\{\mathbf{y}_{t}\}_{t=1}^T)\)</span> for any <span class="math inline">\(\Theta\)</span>. The equality <span class="math inline">\(\ell (\Theta&#39;|\{\mathbf{y}_{t}\}_{t=1}^T) =
Q_y(\Theta&#39; | \Theta&#39;)\)</span> holds if <span class="math inline">\(\Theta = \Theta&#39;\)</span>. Maximizing
Q-function provides an uphill step of the likelihood. Starting from an
initial set of parameters <span class="math inline">\(\hat\Theta_0\)</span>, sparse EM algorithm then
alternates between the expectation step (E-step), where the <span class="math inline">\(Q\)</span>-function <span class="math inline">\(Q_y (\Theta | \hat{\Theta}_{k})\)</span>
conditioning on the parameters <span class="math inline">\(\hat\Theta_{k}\)</span> of the <span class="math inline">\(k\)</span>th iteration is computed, and the
maximization step (M-step), where the parameters are updated by
maximizing the <span class="math inline">\(Q\)</span>-function <span class="math inline">\(\hat{\Theta}_{k+1} = \arg\max_{\Theta} Q_y (\Theta
| \hat{\Theta}_{k})\)</span>.</p>
<p>For the M-step, the maximizer of <span class="math inline">\(Q_y(\Theta | \hat{\Theta}_{k})\)</span> satisfies
that <span class="math inline">\(\frac{1}{T-1} \sum_{t=1}^{T-1}
\mathbf{E}_{t,t+1;k} = \{\frac{1}{T-1}\sum_{t=1}^{T-1}
\mathbf{E}_{t,t;k} \}\mathbf{A}^\top\)</span>, where <span class="math inline">\(\mathbf{E}_{t,s;k} = \mathbb{E} \left\{
\mathbf{x}_{t}\mathbf{x}_{s}^\top |
\{\mathbf{y}_{t&#39;}\}_{t&#39;=1}^T, \hat{\Theta}_{k-1}
\right\}\)</span> for <span class="math inline">\(s, t\in [T]\)</span>
is obtained from the E-step. Instead of directly inverting the matrix
involving <span class="math inline">\(\mathbf{E}_{t,t;k}\)</span>âs,
which is computationally challenging when the dimension <span class="math inline">\(p\)</span> is high and yields a dense estimator of
<span class="math inline">\(\mathbf{A}_*\)</span> leading to a divergent
statistical error, sparse EM algorithm implements generalized Dantzig
selector for Yule-Walker equation,<br />
<span class="math display">\[
\hat{\mathbf{A}}_{k} = \arg\min_{\mathbf{A} \in \mathbb{R}^{p\times
p}}  \|\mathbf{A}\|_1, \;\; \textrm{such that} \;  \left\| \frac{1}{T-1}
\sum_{t=1}^{T-1}   \mathbf{E}_{t,t+1;k} -\frac{1}{T-1} \sum_{t=1}^{T-1}
\mathbf{E}_{t,t;k} \mathbf{A}^\top \right\|_{\max} \le \tau_k,
\]</span> where <span class="math inline">\(\tau_k\)</span> is the
tolerance parameter that is tuned via cross-validation each iteration
(in the observed time series, first <code>Ti_train</code> time points
serve as training set, then gap <code>Ti_gap</code> time points, and use
the remain as test set). The optimization problem <span class="math inline">\(\eqref{eq: sparse_A}\)</span> is solved using
linear programming in a row-by-row parallel fashion. In the package, an
option of further hard thresholding <span class="math inline">\(\hat{\mathbf{A}}_{k}\)</span> is provided to
improve model selection performance. Hard thresholding sets entries of
magnitude less than threshold level as zero. The variance estimates are
next updated as, <span class="math display">\[
\begin{align} \label{eqn: epsilon}
\begin{split}
\hat\sigma_{\eta,k}^2 &amp; =  \frac{1}{p(T-1)} \sum_{t=1}^{T-1} \left\{
\mathrm{tr}( \mathbf{E}_{t+1,t+1;k})  -   \mathrm{tr}\left (
\hat{\mathbf{A}}_{k}  \mathbf{E}_{t,t+1;k} \right) \right\} , \\
\hat\sigma^2_{\epsilon,k} &amp; =  \frac{1}{pT } \sum_{t=1}^{T} \left\{
\mathbf{y}_{t}^\top \mathbf{y}_{t} - 2
\mathbf{y}_{t}^\top  \mathbf{E}_{t;k} + \mathrm{tr} (\mathbf{E}_{t,t;k})
\right\},
\end{split}  
\end{align}
\]</span> where <span class="math inline">\(\mathbf{E}_{t;k} =
\mathbb{E} \{ \mathbf{x}_{t} | \{\mathbf{y}_{t&#39;}\}_{t&#39;=1}^T,
\hat{\Theta}_{k-1} \}\)</span> for <span class="math inline">\(t \in
[T]\)</span>, and <span class="math inline">\(\eqref{eqn:
epsilon}\)</span> comes from taking derivative on <span class="math inline">\(Q_y(\Theta | \hat{\Theta}_{k})\)</span>. Sparse EM
algorithm terminates when reaches the maximal number of iterations or
the estimates are close enough in two consecutive iterations, e.g.,
<span class="math inline">\(\min \left\{ \|\hat{\mathbf{A}}_{k}
-\hat{\mathbf{A}}_{k-1} \|_F , | \hat{\sigma}_{\eta,k
}-\hat{\sigma}_{\eta,k-1}| ,| \hat{\sigma}_{\epsilon,
k}-\hat{\sigma}_{\epsilon, k-1}| \right\} \le 10^{-3}\)</span>.</p>
</div>
<div id="statistical-inference" class="section level2">
<h2>2. Statistical inference</h2>
<div id="a-gaussian-test-statistic-matrix" class="section level4">
<h4>2.a Gaussian test statistic matrix</h4>
<p>The fundamental tools of testing is a gausisan test statistic matrix
whose entries marginally follow standard normal under null.</p>
<p>The test statistic is constructed as follows. Observation <span class="math inline">\(\mathbf{y}_t\)</span> follows an autoregressive
structure, <span class="math inline">\(\mathbf{y}_{t+1} = \mathbf{A}_*
\mathbf{y}_{t} + \mathbf{e}_{t}\)</span>, with the error term <span class="math inline">\(\mathbf{e}_{t} = - \mathbf{A}_*
\mathbf{\epsilon}_{t}+ \mathbf{\epsilon}_{t+1} +
\mathbf{\eta}_{t}\)</span>. Then the lag-1 auto-covariance of the error
<span class="math inline">\(\mathbf{e}_t\)</span> is of the form, <span class="math display">\[
\mathbf{\Sigma}_e = \mathrm{Cov}(\mathbf{e}_{t},\mathbf{e}_{t-1}) =
-\sigma_{\epsilon,*}^2 \mathbf{A}_*.
\]</span> This suggests that we can apply the covariance testing methods
on <span class="math inline">\(\mathbf{\Sigma}_e\)</span> to infer
transition matrix <span class="math inline">\(\mathbf{A}_*\)</span>.
However, <span class="math inline">\(\mathbf{e}_t\)</span> is not
directly observed. Define generic estimates of <span class="math inline">\(\Theta_*\)</span> by <span class="math inline">\(\left \{\hat{\mathbf{A}},\hat\sigma_{\epsilon}^2,
\hat\sigma_{\eta}^2 \right\}\)</span> (sparse EM estimates also work).
We use them to reconstruct this error, and obtain the sample lag-1
auto-covariance estimator, <span class="math display">\[\hat{\mathbf{\Sigma}}_e = \frac{1}{T-2}
\sum_{t=2}^{T-1} \hat{\mathbf{e}}_{t}\hat{\mathbf{e}}_{t-1}^\top, \
\text{where} \ \ \hat{\mathbf{e}}_{t} = \mathbf{y}_{t+1}  -
\hat{\mathbf{A}} \mathbf{y}_{t} - \frac{1}{T-1}\sum_{t&#39;=1}^{T-1}
(\mathbf{y}_{t&#39;+1} -
\hat{\mathbf{A}}\mathbf{y}_{t&#39;}).\]</span></p>
<p>This sample estimator <span class="math inline">\(\hat{\mathbf{\Sigma}}_e\)</span>, nevertheless,
involves some bias due to the reconstruction of the error term, and also
an inflated variance due to the temporal dependence of the time series
data. Bias and variance correction lead to the Gaussian matrix test
statistic <span class="math inline">\(\mathbf{H}\)</span>, whose <span class="math inline">\((i,j)\)</span>th entry is,<br />
<span class="math display">\[
H_{ij} = \frac{ \sum_{t=2}^{T-1}  \{ \hat{e}_{ t,i}\hat{e}_{ t-1,j} +
\left( \hat{\sigma}_{\eta}^2 +\hat{\sigma}_{\epsilon}^2
\right)  \hat{A}_{ij}  - \hat{\sigma}_\eta^2 A_{0,ij} \} }{\sqrt{T-2} \;
\hat{\sigma}_{ij}}, \quad i,j \in [p].
\]</span> <a href="https://arxiv.org/abs/2009.08011">Lyu et
al.Â (2020)</a> proves that, under mild assumptions,<br />
<span class="math display">\[
\frac{ \sum_{t=2}^{T-1}  \{\hat{e}_{ t,i}\hat{e}_{ t-1,j} + \left(
\hat{\sigma}_{\eta}^2 +\hat{\sigma}_{\epsilon}^2
\right)  \hat{A}_{ij}  - \hat{\sigma}_\eta^2 A_{*,ij} \}}{\sqrt{T-2} \;
\hat{\sigma}_{ij}}\rightarrow_{d} \mathrm{N}(0, 1)
\]</span> uniformly for <span class="math inline">\(i,j \in [p]\)</span>
as <span class="math inline">\(p, T \to \infty\)</span>.</p>
</div>
<div id="b-global-testing" class="section level4">
<h4>2.b Global testing</h4>
<p>The key insight of global testing is that the squared maximum entry
of a zero mean normal vector converges to a Gumbel distribution.
Specifically, the global test statistic is<br />
<span class="math display">\[
G_{\mathcal{S}} = \max_{(i,j) \in \mathcal{S}} H_{ij}^2.
\]</span> <a href="https://arxiv.org/abs/2009.08011">Lyu et
al.Â (2020)</a> justifies that the asymptotic null distribution of <span class="math inline">\(G_{\mathcal{S}}\)</span> is Gumbel, <span class="math display">\[
\lim_{|\mathcal{S}| \rightarrow \infty} \mathbb{P} \Big(
G_\mathcal{S}  -2 \log |\mathcal{S}| + \log \log |\mathcal{S}| \le x
\Big) = \exp \left\{- \exp (-x/2) / \sqrt{\pi} \right\}.
\]</span> It leads to an asymptotic <span class="math inline">\(\alpha\)</span>-level test, <span class="math display">\[\begin{eqnarray*}
\Psi_\alpha = \mathbb{1} \big[ G_\mathcal{S} &gt; 2 \log |\mathcal{S}|
-  \log \log |\mathcal{S}| - \log \pi -2 \log\{-\log(1-\alpha)\} \big].
\end{eqnarray*}\]</span> The global null is rejected if <span class="math inline">\(\Psi_\alpha=1\)</span>.</p>
</div>
<div id="c-simultaneous-testing" class="section level4">
<h4>2.c Simultaneous testing</h4>
<p>Let <span class="math inline">\(\mathcal{H}_0 = \{(i,j) :
A_{*,ij}=A_{0,ij}, (i,j) \in \mathcal{S} \}\)</span> denote the set of
true null hypotheses, and <span class="math inline">\(\mathcal{H}_1 = \{
(i,j) : (i,j)\in \mathcal{S} , (i,j) \notin \mathcal{H}_0\}\)</span>
denote the set of true alternatives. The test statistic <span class="math inline">\(H_{ij}\)</span> follows a standard normal
distribution when <span class="math inline">\(H_{0;ij}\)</span> holds,
and as such, we reject <span class="math inline">\(H_{0;ij}\)</span> if
<span class="math inline">\(|H_{ij}| &gt; t\)</span> for some
thresholding value <span class="math inline">\(t &gt; 0\)</span>. Let
<span class="math inline">\(R_{\mathcal{S}}(t) = \sum_{(i,j) \in
\mathcal{S}} \mathbb{1} \{ |H_{ij}|&gt; t\}\)</span> denote the number
of rejections at <span class="math inline">\(t\)</span>. Then the false
discovery proportion (FDP) and the false discovery rate (FDR) in the
simultaneous testing problem are, <span class="math display">\[\begin{eqnarray*}
\textrm{FDP}_{\mathcal{S}}(t)=\frac{\sum_{(i,j) \in \mathcal{H}_0}
\mathbb{1} \{ |H_{ij}|&gt; t\}}{R_{\mathcal{S}}(t)\vee 1}, \;\; \textrm{
and  } \;\;
\textrm{FDR}_{\mathcal{S}}(t) = \mathbb{E} \left\{
\textrm{FDP}_{\mathcal{S}}(t) \right\}.
\end{eqnarray*}\]</span> An ideal choice of the threshold <span class="math inline">\(t\)</span> is to reject as many true positives as
possible, while controlling the false discovery at the pre-specified
level <span class="math inline">\(\beta\)</span>, that is <span class="math inline">\(\inf \{ t &gt; 0 : \text{FDP}_{\mathcal{S}} (t)
\le \beta \}\)</span>. However, <span class="math inline">\(\mathcal{H}_0\)</span> in <span class="math inline">\(\text{FDP}_{\mathcal{S}} (t)\)</span> is unknown.
Observing that <span class="math inline">\(\mathbb{P} ( |H_{ij}|&gt; t )
\approx 2\{ 1- \Phi (t) \}\)</span>, where <span class="math inline">\(\Phi (\cdot)\)</span> is the cumulative
distribution function of a standard normal distribution, the false
rejections <span class="math inline">\(\sum_{(i,j) \in\mathcal{H}_0}
\mathbb{1} \{ |H_{ij}|&gt; t\}\)</span> in <span class="math inline">\(\text{FDP}_{\mathcal{S}} (t)\)</span> can be
approximated by <span class="math inline">\(\{ 2- 2 \Phi(t) \}
|\mathcal{S}|\)</span>. Moreover, the search of <span class="math inline">\(t\)</span> is restricted to the range <span class="math inline">\(\left(0, \sqrt{2\log |\mathcal{S}|}
\right]\)</span>, since <span class="math inline">\(\mathbb{P}\left(
\hat{t} \text{ exists in } \left(0, \sqrt{2\log |\mathcal{S}|}\right]
\right) \to 1\)</span> as shown in the theoretical justification of <a href="https://arxiv.org/abs/2009.08011">Lyu et al.Â (2020)</a>. The
simultaneous testing procedure is justified that consistently control
FDR, <span class="math display">\[
\lim_{|\mathcal{S}| \to \infty} \frac{\text{FDR}_{\mathcal{S}} (\,
\hat{t} \; )}{\beta |\mathcal{H}_0|/|\mathcal{S}|} = 1, \quad \textrm{
and } \quad
\frac{\text{FDP}_{\mathcal{S}} (\, \hat{t} \; )}{\beta |
\mathcal{H}_0|/|\mathcal{S}|}\rightarrow_{p} 1 \;\;  \textrm{ as } \;
|\mathcal{S}| \to \infty.
\]</span></p>
<p><a id="example"></a></p>
</div>
</div>
</div>
<div id="quick-start" class="section level1">
<h1>Quick start</h1>
<p>The purpose of this section is to show users the basic usage of this
package. We will briefly go through main functions, see what they can do
and have a look at outputs. An detailed example of complete procedures
of estimation and inference is be presented to give users a general
sense of the pakcage.</p>
<p>We first generate observations from the model.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(hdiVAR)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>p<span class="ot">=</span><span class="dv">3</span>; Ti<span class="ot">=</span><span class="dv">400</span>  <span class="co"># dimension and time</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>A<span class="ot">=</span><span class="fu">diag</span>(<span class="dv">1</span>,p) <span class="co"># transition matrix</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>sig_eta<span class="ot">=</span>sig_epsilon<span class="ot">=</span><span class="fl">0.2</span> <span class="co"># error std</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>Y<span class="ot">=</span><span class="fu">array</span>(<span class="dv">0</span>,<span class="at">dim=</span><span class="fu">c</span>(p,Ti)) <span class="co">#observation t=1, ...., Ti</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>X<span class="ot">=</span><span class="fu">array</span>(<span class="dv">0</span>,<span class="at">dim=</span><span class="fu">c</span>(p,Ti)) <span class="co">#latent t=1, ...., T</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>Ti_burnin<span class="ot">=</span><span class="dv">400</span> <span class="co"># time for burn-in to stationarity</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (t <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>(Ti<span class="sc">+</span>Ti_burnin)) {</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a> <span class="cf">if</span> (t<span class="sc">==</span><span class="dv">1</span>){</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>   x1<span class="ot">=</span><span class="fu">rnorm</span>(p)</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a> } <span class="cf">else</span> <span class="cf">if</span> (t<span class="sc">&lt;=</span>Ti_burnin) { <span class="co"># burn in</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>   x1<span class="ot">=</span>A<span class="sc">%*%</span>x1<span class="sc">+</span><span class="fu">rnorm</span>(p,<span class="at">mean=</span><span class="dv">0</span>,<span class="at">sd=</span>sig_eta)</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a> } <span class="cf">else</span> <span class="cf">if</span> (t<span class="sc">==</span>(Ti_burnin<span class="sc">+</span><span class="dv">1</span>)){ <span class="co"># time series used for learning</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>   X[,t<span class="sc">-</span>Ti_burnin]<span class="ot">=</span>x1</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>   Y[,t<span class="sc">-</span>Ti_burnin]<span class="ot">=</span>X[,t<span class="sc">-</span>Ti_burnin]<span class="sc">+</span><span class="fu">rnorm</span>(p,<span class="at">mean=</span><span class="dv">0</span>,<span class="at">sd=</span>sig_epsilon)</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a> } <span class="cf">else</span> {</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>   X[,t<span class="sc">-</span> Ti_burnin]<span class="ot">=</span>A<span class="sc">%*%</span>X[,t<span class="dv">-1</span><span class="sc">-</span> Ti_burnin]<span class="sc">+</span><span class="fu">rnorm</span>(p,<span class="at">mean=</span><span class="dv">0</span>,<span class="at">sd=</span>sig_eta)</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>   Y[,t<span class="sc">-</span> Ti_burnin]<span class="ot">=</span>X[,t<span class="sc">-</span> Ti_burnin]<span class="sc">+</span><span class="fu">rnorm</span>(p,<span class="at">mean=</span><span class="dv">0</span>,<span class="at">sd=</span>sig_epsilon)</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a> }</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p>The first example is sparse EM algorithm.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># cross-validation grid of tolerance parameter \tau_k in Dantzig selector.</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>tol_seq<span class="ot">=</span><span class="fu">c</span>(<span class="fl">0.0001</span>,<span class="fl">0.0003</span>,<span class="fl">0.0005</span>) </span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="co"># cross-validation grid of hard thresholding levels in transition matrix estimate. </span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Set as zero to avoid thresholding. The output is \hat{A}_k.</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>ht_seq<span class="ot">=</span><span class="dv">0</span> </span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>A_init<span class="ot">=</span><span class="fu">diag</span>(<span class="fl">0.1</span>,p) <span class="co"># initial estimate of A </span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a><span class="co"># initial estimates of error variances </span></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>sig2_eta_init<span class="ot">=</span>sig2_epsilon_init<span class="ot">=</span><span class="fl">0.1</span> </span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a><span class="co"># the first half time points are training data </span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>Ti_train<span class="ot">=</span>Ti<span class="sc">*</span><span class="fl">0.5</span></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a><span class="co"># The latter 3/10 time points are test data (drop out train (1/2) and gap (1/5) sets).</span></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>Ti_gap<span class="ot">=</span>Ti<span class="sc">*</span><span class="fl">0.2</span> </span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a><span class="co"># sparse EM algorithm </span></span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>sEM_fit<span class="ot">=</span><span class="fu">sEM</span>(Y,A_init,sig2_eta_init,sig2_epsilon_init,Ti_train,Ti_gap,tol_seq,ht_seq,<span class="at">is_echo =</span> <span class="cn">TRUE</span>)</span></code></pre></div>
<pre><code>## [1] &quot;CV-tuned (tol, ht) is in (1, 1)/(3, 1) of the parameter grid.&quot;
## [1] &quot;CV-tuned (tol, ht) is in (1, 1)/(3, 1) of the parameter grid.&quot;
## [1] &quot;CV-tuned (tol, ht) is in (1, 1)/(3, 1) of the parameter grid.&quot;
## [1] &quot;CV-tuned (tol, ht) is in (1, 1)/(3, 1) of the parameter grid.&quot;
## [1] &quot;CV-tuned (tol, ht) is in (1, 1)/(3, 1) of the parameter grid.&quot;
## sparse EM is terminated due to vanishing updates</code></pre>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># estimate of A </span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>sEM_fit<span class="sc">$</span>A_est </span></code></pre></div>
<pre><code>##            [,1]         [,2]        [,3]
## [1,] 0.96419601 -0.026679552 0.012691775
## [2,] 0.01440378  0.985762599 0.006283281
## [3,] 0.01642908  0.009632632 0.995205126</code></pre>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># estimate of error variances </span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="fu">c</span>(sEM_fit<span class="sc">$</span>sig2_epsilon_hat,sEM_fit<span class="sc">$</span>sig2_eta_hat) </span></code></pre></div>
<pre><code>## [1] 0.06429002 0.05989768</code></pre>
<p>The second example is statistical inference.</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># use sparse EM estimates to construct test. Alternative consistent estimators can also be adopted if any. </span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="co"># test the entire matrix.</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a><span class="co"># FDR control levels for simultaneous testing </span></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>FDR_levels<span class="ot">=</span><span class="fu">c</span>(<span class="fl">0.05</span>,<span class="fl">0.1</span>)</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a><span class="co"># if null hypotheses are true (null hypothesis is true A): </span></span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a><span class="co"># p-value should &gt; 0.05, and simultaneous testing selects no entries. </span></span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>true_null<span class="ot">=</span><span class="fu">hdVARtest</span>(Y,sEM_fit<span class="sc">$</span>A_est,sEM_fit<span class="sc">$</span>sig2_eta_hat,sEM_fit<span class="sc">$</span>sig2_epsilon_hat,</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>                    <span class="at">global_H0=</span>A,<span class="at">global_idx=</span><span class="cn">NULL</span>,<span class="at">simul_H0=</span>A,</span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>                    <span class="at">simul_idx=</span><span class="cn">NULL</span>,<span class="at">FDR_levels=</span>FDR_levels)</span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a><span class="co"># global pvalue: </span></span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a>true_null<span class="sc">$</span>pvalue</span></code></pre></div>
<pre><code>## [1] 0.4223181</code></pre>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co"># selection at FDR=0.05 control level </span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>true_null<span class="sc">$</span>selected[,,FDR_levels<span class="sc">==</span><span class="fl">0.05</span>]</span></code></pre></div>
<pre><code>##      [,1] [,2] [,3]
## [1,]    0    0    0
## [2,]    0    0    0
## [3,]    0    0    0</code></pre>
<div class="sourceCode" id="cb12"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co"># if null hypotheses are false (null hypothesis is zero matrix): </span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="co"># p-value should &lt; 0.05, and simultaneous testing selects diagnoal entries. </span></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>false_null<span class="ot">=</span><span class="fu">hdVARtest</span>(Y,sEM_fit<span class="sc">$</span>A_est,sEM_fit<span class="sc">$</span>sig2_eta_hat,sEM_fit<span class="sc">$</span>sig2_epsilon_hat,</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>                     <span class="at">global_H0=</span><span class="fu">matrix</span>(<span class="dv">0</span>,p,p),<span class="at">global_idx=</span><span class="cn">NULL</span>,<span class="at">simul_H0=</span><span class="fu">matrix</span>(<span class="dv">0</span>,p,p),</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>                     <span class="at">simul_idx=</span><span class="cn">NULL</span>,<span class="at">FDR_levels=</span><span class="fu">c</span>(<span class="fl">0.05</span>,<span class="fl">0.1</span>))</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a><span class="co"># global pvalue: </span></span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>false_null<span class="sc">$</span>pvalue</span></code></pre></div>
<pre><code>## [1] 3.477663e-12</code></pre>
<div class="sourceCode" id="cb14"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="co"># selection at FDR=0.05 control level </span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>false_null<span class="sc">$</span>selected[,,FDR_levels<span class="sc">==</span><span class="fl">0.05</span>]</span></code></pre></div>
<pre><code>##      [,1] [,2] [,3]
## [1,]    1    0    0
## [2,]    0    1    0
## [3,]    0    0    1</code></pre>
<p><a href="#top">Back to Top</a></p>
<p><a id="ref"></a></p>
</div>
<div id="reference" class="section level1">
<h1>Reference</h1>
<ol style="list-style-type: decimal">
<li>Xiang Lyu, Jian Kang, and Lexin Li. <em>Statistical Inference for
High-Dimensional Vector Autoregression with Measurement Error.</em>
<strong><em>arXiv preprint</em></strong> <strong>arXiv:2009.08011
(2020)</strong>.</li>
</ol>
</div>



<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
